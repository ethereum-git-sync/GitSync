{
  "url": "https://api.github.com/repos/ethereum/go-ethereum/issues/21310",
  "repository_url": "https://api.github.com/repos/ethereum/go-ethereum",
  "labels_url": "https://api.github.com/repos/ethereum/go-ethereum/issues/21310/labels{/name}",
  "comments_url": "https://api.github.com/repos/ethereum/go-ethereum/issues/21310/comments",
  "events_url": "https://api.github.com/repos/ethereum/go-ethereum/issues/21310/events",
  "html_url": "https://github.com/ethereum/go-ethereum/issues/21310",
  "id": 653516757,
  "node_id": "MDU6SXNzdWU2NTM1MTY3NTc=",
  "number": 21310,
  "title": "Snapshot performance predictability",
  "user": {
    "login": "AusIV",
    "id": 977954,
    "node_id": "MDQ6VXNlcjk3Nzk1NA==",
    "avatar_url": "https://avatars.githubusercontent.com/u/977954?v=4",
    "gravatar_id": "",
    "url": "https://api.github.com/users/AusIV",
    "html_url": "https://github.com/AusIV",
    "followers_url": "https://api.github.com/users/AusIV/followers",
    "following_url": "https://api.github.com/users/AusIV/following{/other_user}",
    "gists_url": "https://api.github.com/users/AusIV/gists{/gist_id}",
    "starred_url": "https://api.github.com/users/AusIV/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/AusIV/subscriptions",
    "organizations_url": "https://api.github.com/users/AusIV/orgs",
    "repos_url": "https://api.github.com/users/AusIV/repos",
    "events_url": "https://api.github.com/users/AusIV/events{/privacy}",
    "received_events_url": "https://api.github.com/users/AusIV/received_events",
    "type": "User",
    "site_admin": false
  },
  "labels": [
    {
      "id": 1671673325,
      "node_id": "MDU6TGFiZWwxNjcxNjczMzI1",
      "url": "https://api.github.com/repos/ethereum/go-ethereum/labels/status:work-in-progress",
      "name": "status:work-in-progress",
      "color": "6bb6e5",
      "default": false,
      "description": ""
    }
  ],
  "state": "open",
  "locked": false,
  "assignee": null,
  "assignees": [

  ],
  "milestone": null,
  "comments": 7,
  "created_at": "2020-07-08T18:45:25Z",
  "updated_at": "2020-09-17T08:41:53Z",
  "closed_at": null,
  "author_association": "CONTRIBUTOR",
  "active_lock_reason": null,
  "body": "#### System information\r\n\r\nGeth version: `1.9.14-stable`\r\nOS & Version: Linux\r\n\r\n#### Expected behaviour\r\n\r\nThe performance benefits of the snapshot system introduced in 1.9.13 are pretty phenomenal - We've seen some eth_call queries go from 5+ seconds to well under a second with a fully loaded snapshot in place. Running an operation where capacity planning relies on predictable performance, we'd love to be able to take advantage of the snapshot, but to do so requires consistent, predictable performance.\r\n\r\n#### Actual behaviour\r\n\r\nIn the event of an abrupt termination of Geth, the snapshot journal is not written to disk. On the next startup, because the journal is missing, the snapshot is wiped and begins regenerating. On the Ethereum mainnet, snapshot generation can take several days, during which performance is considerably below the peak performance operating with a completed snapshot.\r\n\r\n\r\n#### Steps to reproduce the behaviour\r\n\r\nkill -9 $GETH_PID\r\n\r\nRestart Geth\r\n\r\n#### Proposed Solution\r\n\r\nIn our case, it would be very preferable to have a longer startup time but have servers come online with working snapshots, rather than come online quickly but spend days reconstructing the snapshot. There are two possible ways to do this:\r\n\r\n1. Our operation typically runs with `--gcmode=archive`. Given that we have all of the intermediate state tries between the 128 block old diskLayer and the current block, we should be able to reconstruct the 128 diffLayers by comparing the state tries while initializing the snapshot. We'd still use the journal for clean shutdowns, as it's much faster than reconstructing the diffLayer by comparing state tries.\r\n2. For those who don't run with `--gcmode=archive` (which I believe is most users), it may be necessary to roll the current block back to the same block as the diskLayer, and resume syncing from there. I'm not sure how feasible this is, as it seems the snapshot tree calls Cap(root, 127) every block, while the trie only gets synced to disk when the dirty threshold is exceeded, and it seems unlikely that those events are likely to line up. If we could ensure that commits to the disk layer happen at the same time as the trie gets committed to disk, then unclean shutdowns should be able to resume from just the disk layer without having to discard the snapshot. Again, we'd still use the journal for clean shutdowns as that makes it easier to handle possible reorgs, but this approach gives us a reasonable hope of not having to discard the snapshot after an unclean shutdown.\r\n\r\nI've already been working on a proof-of-concept of option 1 - once I have the kinks worked out I'd be happy to push it upstream if you'd be amenable. I'd also be willing to hash out a PR for option 2 if that's something you'd be open to, but since I don't need it myself I don't want to sink the time into it if the Geth team has other plans.",
  "closed_by": null,
  "reactions": {
    "url": "https://api.github.com/repos/ethereum/go-ethereum/issues/21310/reactions",
    "total_count": 0,
    "+1": 0,
    "-1": 0,
    "laugh": 0,
    "hooray": 0,
    "confused": 0,
    "heart": 0,
    "rocket": 0,
    "eyes": 0
  },
  "timeline_url": "https://api.github.com/repos/ethereum/go-ethereum/issues/21310/timeline",
  "performed_via_github_app": null,
  "state_reason": null
}
[
  {
    "url": "https://api.github.com/repos/ethereum/go-ethereum/issues/comments/657461328",
    "html_url": "https://github.com/ethereum/go-ethereum/issues/21310#issuecomment-657461328",
    "issue_url": "https://api.github.com/repos/ethereum/go-ethereum/issues/21310",
    "id": 657461328,
    "node_id": "MDEyOklzc3VlQ29tbWVudDY1NzQ2MTMyOA==",
    "user": {
      "login": "holiman",
      "id": 142290,
      "node_id": "MDQ6VXNlcjE0MjI5MA==",
      "avatar_url": "https://avatars.githubusercontent.com/u/142290?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/holiman",
      "html_url": "https://github.com/holiman",
      "followers_url": "https://api.github.com/users/holiman/followers",
      "following_url": "https://api.github.com/users/holiman/following{/other_user}",
      "gists_url": "https://api.github.com/users/holiman/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/holiman/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/holiman/subscriptions",
      "organizations_url": "https://api.github.com/users/holiman/orgs",
      "repos_url": "https://api.github.com/users/holiman/repos",
      "events_url": "https://api.github.com/users/holiman/events{/privacy}",
      "received_events_url": "https://api.github.com/users/holiman/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2020-07-13T10:01:59Z",
    "updated_at": "2020-07-13T10:01:59Z",
    "author_association": "MEMBER",
    "body": "Nice to hear you're experimenting with snapshot! And also good to hear it's \"pretty phenomenal\", thanks!\r\n\r\nI agree that there should be a way to start geth in \"finish up the snapshot, _then_ to the blockchain stuff\". \r\nRegarding your proposals: \r\n\r\nThe problem is not so much that we need to reconstruct 128 layers, geth can handle it fine if those layers are missing (except that we won't be able to handle reorgs nicely). Actually, after a snapshot is newly generated, those memory layers are not even present -- there's only the disklayer. \r\n\r\nI want to clarify one thing re flushing to disk. So, there are only 127 memory-layers (or 128?), but those do not map 1:1 to 128 previous states/blocks. They map to _at minimum_ 128 blocks, but we merge into the bottom-most memory layer, so that layer can represent N (maybe 100) actual layers. So the guarantee is that we can reorg to any of the latest 128 states, but we accumulate older blocks into the bottom-most mem layer, and every once in a while flush that one to disk. So\r\n\r\nThere are a couple of things that could be done: \r\n\r\n1. Create snapshot before starting blockchain would be a good feature. \r\n2. Flush snapshots to disk every time we also flush the trie. However, this won't work well, since the next snapshot-flush will un-align these anyway. \r\n3. Flush trie to disk every time we flush the snapshot. This won't work well either, since it'll screw up the pruning, and eat even more disk space.\r\n\r\nAnyway, I think your proposal number 1 won't really work, because it's not sufficient to create 128 block-states on top of the disklayer -- but if you go through _all_ block-states and merge the bottom ones into the bottom layer, then it could work. ",
    "reactions": {
      "url": "https://api.github.com/repos/ethereum/go-ethereum/issues/comments/657461328/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "performed_via_github_app": null
  },
  {
    "url": "https://api.github.com/repos/ethereum/go-ethereum/issues/comments/657589987",
    "html_url": "https://github.com/ethereum/go-ethereum/issues/21310#issuecomment-657589987",
    "issue_url": "https://api.github.com/repos/ethereum/go-ethereum/issues/21310",
    "id": 657589987,
    "node_id": "MDEyOklzc3VlQ29tbWVudDY1NzU4OTk4Nw==",
    "user": {
      "login": "AusIV",
      "id": 977954,
      "node_id": "MDQ6VXNlcjk3Nzk1NA==",
      "avatar_url": "https://avatars.githubusercontent.com/u/977954?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/AusIV",
      "html_url": "https://github.com/AusIV",
      "followers_url": "https://api.github.com/users/AusIV/followers",
      "following_url": "https://api.github.com/users/AusIV/following{/other_user}",
      "gists_url": "https://api.github.com/users/AusIV/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/AusIV/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/AusIV/subscriptions",
      "organizations_url": "https://api.github.com/users/AusIV/orgs",
      "repos_url": "https://api.github.com/users/AusIV/repos",
      "events_url": "https://api.github.com/users/AusIV/events{/privacy}",
      "received_events_url": "https://api.github.com/users/AusIV/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2020-07-13T14:21:06Z",
    "updated_at": "2020-07-13T14:21:06Z",
    "author_association": "CONTRIBUTOR",
    "body": "Sorry, when I said \"reconstruct 128 layers\" what I really meant is to reconstruct the layers between the diskLayer and the latest block - in my experimenting that has been 128 layers, but I'm also working with my fork of Geth that does streaming replication, and I don't think we're consolidating changes in the bottom-most memory layer - we're just merging the bottom layer to disk with each successive block (we'll need to revisit that separately). The pseudocode of what I've done is basically:\r\n\r\n```\r\nheader = ReadHeadHeader()\r\nroot = GetSnapshotRoot(diskdb)\r\nintermediateRoots = []common.Hash{header.Root}\r\nfor header.Root != root {\r\n    header = ReadHeader(header.ParentHash)\r\n    intermediateRoots = append([]common.Hash{header.Root}, intermediateRoots...)\r\n}\r\n\r\nparentRoot = root\r\nfor i = 1; i < len(intermediateRoots); i++ {\r\n    destructs, accounts, storage, err = DiffTries(intermediateRoots[i-1], intermediateRoots[i])\r\n    // handle error\r\n    snaps.Update(intermediateRoots[i], intermediateRoots[i-1], destructs, accounts, storage)\r\n}\r\n```\r\n\r\nSo I'm not hard coding anything to 128 blocks - it's just that in my experience with my replication system that number has been 128. \r\n\r\nAs far as your list of options: Number 1 is useful for initializing things, but in my experience generating an initial snapshot from scratch takes several days, so isn't an answer when you're spinning up instances to deal with capacity limitations. Would it not be possible to combine options 2 and 3? If we synchronize the snapshot flushes with the trie writes such that they all get written at once (never flushing the trie to disk without flushing the snapshot and never flushing the snapshot to disk without flushing the trie). Is it necessary to to flush the snapshots to disk more often than we're flushing the trie to disk? If we can keep those two things in sync we can avoid having to rebuild the snapshot or suffering a performance degradation as it rebuilds.",
    "reactions": {
      "url": "https://api.github.com/repos/ethereum/go-ethereum/issues/comments/657589987/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "performed_via_github_app": null
  },
  {
    "url": "https://api.github.com/repos/ethereum/go-ethereum/issues/comments/658581594",
    "html_url": "https://github.com/ethereum/go-ethereum/issues/21310#issuecomment-658581594",
    "issue_url": "https://api.github.com/repos/ethereum/go-ethereum/issues/21310",
    "id": 658581594,
    "node_id": "MDEyOklzc3VlQ29tbWVudDY1ODU4MTU5NA==",
    "user": {
      "login": "karalabe",
      "id": 129561,
      "node_id": "MDQ6VXNlcjEyOTU2MQ==",
      "avatar_url": "https://avatars.githubusercontent.com/u/129561?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/karalabe",
      "html_url": "https://github.com/karalabe",
      "followers_url": "https://api.github.com/users/karalabe/followers",
      "following_url": "https://api.github.com/users/karalabe/following{/other_user}",
      "gists_url": "https://api.github.com/users/karalabe/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/karalabe/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/karalabe/subscriptions",
      "organizations_url": "https://api.github.com/users/karalabe/orgs",
      "repos_url": "https://api.github.com/users/karalabe/repos",
      "events_url": "https://api.github.com/users/karalabe/events{/privacy}",
      "received_events_url": "https://api.github.com/users/karalabe/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2020-07-15T06:52:30Z",
    "updated_at": "2020-07-15T06:52:30Z",
    "author_association": "MEMBER",
    "body": "As Martin mentioned, Option 1 will not work because the disk layer might be older than 128. It would get a bit messy and I don't want to rely on - or have special casing - for archive nodes.\r\n\r\nOption 2 is much more interesting though. Even if the persistent disk layer is lets say 200-300 blocks old, we *could* rerun those blocks on startup. Although the state trie is already pruned, we *know* that the blocks we have in the database are all already validated, so we could just run via the snapshots and skip merkle root checks + trie commits for any blocks that we don't have the trie for. This would allow us to even faster crunch through the blocks and reconstruct the missing diff layers.",
    "reactions": {
      "url": "https://api.github.com/repos/ethereum/go-ethereum/issues/comments/658581594/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "performed_via_github_app": null
  },
  {
    "url": "https://api.github.com/repos/ethereum/go-ethereum/issues/comments/658583997",
    "html_url": "https://github.com/ethereum/go-ethereum/issues/21310#issuecomment-658583997",
    "issue_url": "https://api.github.com/repos/ethereum/go-ethereum/issues/21310",
    "id": 658583997,
    "node_id": "MDEyOklzc3VlQ29tbWVudDY1ODU4Mzk5Nw==",
    "user": {
      "login": "karalabe",
      "id": 129561,
      "node_id": "MDQ6VXNlcjEyOTU2MQ==",
      "avatar_url": "https://avatars.githubusercontent.com/u/129561?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/karalabe",
      "html_url": "https://github.com/karalabe",
      "followers_url": "https://api.github.com/users/karalabe/followers",
      "following_url": "https://api.github.com/users/karalabe/following{/other_user}",
      "gists_url": "https://api.github.com/users/karalabe/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/karalabe/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/karalabe/subscriptions",
      "organizations_url": "https://api.github.com/users/karalabe/orgs",
      "repos_url": "https://api.github.com/users/karalabe/repos",
      "events_url": "https://api.github.com/users/karalabe/events{/privacy}",
      "received_events_url": "https://api.github.com/users/karalabe/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2020-07-15T06:58:17Z",
    "updated_at": "2020-07-15T06:58:17Z",
    "author_association": "MEMBER",
    "body": "Hmm, that said, I do see the issue with the trie/snap desync. If I murder Geth, I could recreate the snapshot diffs easily but the trie might be completely missing. Hmm.\r\n\r\nWe can't afford to keep that many diff layers in RAM. Tries are pruned on the fly and are pushed to disk rarely in their entirety (maybe once a day). That works ok-ish because if we run out of memory allowance, we can just flush overflow items which are old, still keeping data consistent. In the case of the snapshots, we can't flush just bits here and there, we'd need to push entire layers to keep the entire disk layer consistent. ",
    "reactions": {
      "url": "https://api.github.com/repos/ethereum/go-ethereum/issues/comments/658583997/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "performed_via_github_app": null
  },
  {
    "url": "https://api.github.com/repos/ethereum/go-ethereum/issues/comments/658586866",
    "html_url": "https://github.com/ethereum/go-ethereum/issues/21310#issuecomment-658586866",
    "issue_url": "https://api.github.com/repos/ethereum/go-ethereum/issues/21310",
    "id": 658586866,
    "node_id": "MDEyOklzc3VlQ29tbWVudDY1ODU4Njg2Ng==",
    "user": {
      "login": "karalabe",
      "id": 129561,
      "node_id": "MDQ6VXNlcjEyOTU2MQ==",
      "avatar_url": "https://avatars.githubusercontent.com/u/129561?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/karalabe",
      "html_url": "https://github.com/karalabe",
      "followers_url": "https://api.github.com/users/karalabe/followers",
      "following_url": "https://api.github.com/users/karalabe/following{/other_user}",
      "gists_url": "https://api.github.com/users/karalabe/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/karalabe/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/karalabe/subscriptions",
      "organizations_url": "https://api.github.com/users/karalabe/orgs",
      "repos_url": "https://api.github.com/users/karalabe/repos",
      "events_url": "https://api.github.com/users/karalabe/events{/privacy}",
      "received_events_url": "https://api.github.com/users/karalabe/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2020-07-15T07:05:13Z",
    "updated_at": "2020-07-15T07:05:13Z",
    "author_association": "MEMBER",
    "body": "Perhaps a hard but doable solution would be to explicitly detect if the snapshot is available, but desynced, and in that case roll back the blocks to the most recent available state, reprocess all the blocks via trie only; then go back to the most recent available snapshot and reprocess snapshot only. It's annoying, but it would be infinitely faster than it is now (you need to reprocess the trie anyway, so no difference there, but the snapshot could be salvaged).",
    "reactions": {
      "url": "https://api.github.com/repos/ethereum/go-ethereum/issues/comments/658586866/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "performed_via_github_app": null
  },
  {
    "url": "https://api.github.com/repos/ethereum/go-ethereum/issues/comments/659690202",
    "html_url": "https://github.com/ethereum/go-ethereum/issues/21310#issuecomment-659690202",
    "issue_url": "https://api.github.com/repos/ethereum/go-ethereum/issues/21310",
    "id": 659690202,
    "node_id": "MDEyOklzc3VlQ29tbWVudDY1OTY5MDIwMg==",
    "user": {
      "login": "AusIV",
      "id": 977954,
      "node_id": "MDQ6VXNlcjk3Nzk1NA==",
      "avatar_url": "https://avatars.githubusercontent.com/u/977954?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/AusIV",
      "html_url": "https://github.com/AusIV",
      "followers_url": "https://api.github.com/users/AusIV/followers",
      "following_url": "https://api.github.com/users/AusIV/following{/other_user}",
      "gists_url": "https://api.github.com/users/AusIV/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/AusIV/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/AusIV/subscriptions",
      "organizations_url": "https://api.github.com/users/AusIV/orgs",
      "repos_url": "https://api.github.com/users/AusIV/repos",
      "events_url": "https://api.github.com/users/AusIV/events{/privacy}",
      "received_events_url": "https://api.github.com/users/AusIV/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2020-07-16T21:41:57Z",
    "updated_at": "2020-07-16T21:41:57Z",
    "author_association": "CONTRIBUTOR",
    "body": "That makes a lot of sense. To restate, the basic idea would be to identify that the snapshot is ahead of the state trie and set it aside until the block processing catches up to it, then once caught up resume updating the snapshot?\r\n\r\nThat would still require a special case for archive nodes, as in the case where Geth dies abruptly the trie will be ahead of the snapshot rather than behind it, though that doesn't necessarily have to be as complex a special case as I described earlier - it would just need to roll back the block to match the snapshot and resume from there rather try to bring the snapshot up to date.\r\n\r\nPerhaps you could clarify one other thing for me - when a Geth node dies abruptly, upon restart it resets the head to the last available state, then reacquires subsequent blocks from peers, correct? It doesn't reprocess the blocks stored in the database, does it?",
    "reactions": {
      "url": "https://api.github.com/repos/ethereum/go-ethereum/issues/comments/659690202/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "performed_via_github_app": null
  },
  {
    "url": "https://api.github.com/repos/ethereum/go-ethereum/issues/comments/688668903",
    "html_url": "https://github.com/ethereum/go-ethereum/issues/21310#issuecomment-688668903",
    "issue_url": "https://api.github.com/repos/ethereum/go-ethereum/issues/21310",
    "id": 688668903,
    "node_id": "MDEyOklzc3VlQ29tbWVudDY4ODY2ODkwMw==",
    "user": {
      "login": "holiman",
      "id": 142290,
      "node_id": "MDQ6VXNlcjE0MjI5MA==",
      "avatar_url": "https://avatars.githubusercontent.com/u/142290?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/holiman",
      "html_url": "https://github.com/holiman",
      "followers_url": "https://api.github.com/users/holiman/followers",
      "following_url": "https://api.github.com/users/holiman/following{/other_user}",
      "gists_url": "https://api.github.com/users/holiman/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/holiman/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/holiman/subscriptions",
      "organizations_url": "https://api.github.com/users/holiman/orgs",
      "repos_url": "https://api.github.com/users/holiman/repos",
      "events_url": "https://api.github.com/users/holiman/events{/privacy}",
      "received_events_url": "https://api.github.com/users/holiman/received_events",
      "type": "User",
      "site_admin": false
    },
    "created_at": "2020-09-08T07:13:44Z",
    "updated_at": "2020-09-08T07:13:44Z",
    "author_association": "MEMBER",
    "body": "Coming back to this, sorry for the delay.\r\n\r\n> Perhaps you could clarify one other thing for me - when a Geth node dies abruptly, upon restart it resets the head to the last available state, then reacquires subsequent blocks from peers, correct? It doesn't reprocess the blocks stored in the database, does it?\r\n\r\nAs of https://github.com/ethereum/go-ethereum/pull/21409, the behaviour upon restart with various types of corrupt data is a lot more well defined. I think your description matches the behaviour, basically:\r\n\r\n- reset to last available state, \r\n- prune 'future' blocks if needed, \r\n- fetch blocks from network\r\n\r\n@karalabe correct me if I'm wrong",
    "reactions": {
      "url": "https://api.github.com/repos/ethereum/go-ethereum/issues/comments/688668903/reactions",
      "total_count": 0,
      "+1": 0,
      "-1": 0,
      "laugh": 0,
      "hooray": 0,
      "confused": 0,
      "heart": 0,
      "rocket": 0,
      "eyes": 0
    },
    "performed_via_github_app": null
  }
]
